{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Audio, Speech, Vision Processing Lab - Emotional Sound Database (ASVP-ESD)\n",
    "\n",
    "## Description\n",
    "The dataset contains 12.625 audio files. It contains speech and non-speech emotional sound. The data (audio) was collected from movies, tv shows, youtube, and other websites.\n",
    "\n",
    "## Emotion Classes\n",
    "\n",
    "The dataset contains a total of 12 distinct emotions:\n",
    "\n",
    "* Boredome\n",
    "* Neutral\n",
    "* Happiness\n",
    "* Sadness\n",
    "* Anger\n",
    "* Fear\n",
    "* Surprise\n",
    "* Disgust\n",
    "* Excite\n",
    "* Pleasure\n",
    "* Pain\n",
    "\n",
    "## File Naming\n",
    "\n",
    "Each audio file has a unique filename. The filename consists of numberical identifiers (e.g. 02-01-06-01-02-105-02-01-02.wav) these identifiers define the stimulus characteristic.\n",
    "\n",
    "### Filename Identifiers\n",
    "\n",
    "1. Modality - (03 = audio-only)\n",
    "2. Vocal Channel - (01 = speech, 02 = non speech)\n",
    "3. Emotion - (01 = boredom/sigh, 02 = neutral/calm, 03 = happy/laugh/gaggle, 04 = sad/cry, 05 = angry/grunt/frustation, 06 = fearful/scream/panic, 07 = disgust/dislike/contempt, 08 = surprised/gasp/amazed, 09 = excited, 10 = pleasure, 11 = pain/groan, 12 = disappointment/disapproval, 13 = breath)\n",
    "4. Emotional Intensity - (01 = normal, 02 = high)\n",
    "5. Statement - (as it's non scripted this help to refer approcimately to data collected from the same period or source base on their rank)\n",
    "6. Actor - (even number represent male, odd numbers represent female)\n",
    "7. Age - (01 = above 65 years, 02 = above 20 , 03 = below 20 years)\n",
    "8. Source - (01 & 02 = movies/youtube/website, 03 = movies)\n",
    "9. Language - (01 = Chinese, 02 = English, 04 = French, others = russian/others)\n",
    "\n",
    "Exaple: 03-01-06-01-02-12-02-01-02-16.wav = audio_only-speech-fearful/scream/panic-normal-statement-male_actor_12-(20, 65)-movies/youtube/website-english-similar_with_16_other_audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "from enum import Enum\n",
    "from typing import Literal, List\n",
    "from uuid import uuid4\n",
    "import pathlib\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "The first step is to filter the audio files to the sub set of audio files that we are interested in. \n",
    "\n",
    "We are interested in the following characteristics:\n",
    "\n",
    "* Speech or (just) Sound\n",
    "* Enlish Language\n",
    "\n",
    "We are interested in the following emotions:\n",
    "\n",
    "* Happy\n",
    "* Excited\n",
    "* Tender\n",
    "* Scared\n",
    "* Angry\n",
    "* Sad\n",
    "\n",
    "It is needed to take steps to transform the \"raw\" data into a new sub set that meet these criteria. This step needs to be done first before it is possible to move on data analysis, or any other data related actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firt, let's define the various types of identifiers as enums for easy use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base functionality of enum\n",
    "class BaseEnum(str, Enum):\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.value\n",
    "\n",
    "\n",
    "# the medium in which emotion is conveyed\n",
    "class Modality(BaseEnum):\n",
    "\n",
    "    AUDIO_ONLY = \"03\"\n",
    "\n",
    "\n",
    "# the type of audio (speech, or non speech)\n",
    "class VocalChannel(BaseEnum):\n",
    "\n",
    "    SPEECH = \"01\"\n",
    "    NON_SPEECH = \"02\"\n",
    "\n",
    "\n",
    "# the type of emotion, this will be reduced to the base six methioned previously\n",
    "class Emotion(BaseEnum):\n",
    "\n",
    "    BOREDOM = \"01\"\n",
    "    NEUTRAL = \"02\"\n",
    "    HAPPY = \"03\"\n",
    "    SAD = \"04\"\n",
    "    ANGRY = \"05\"\n",
    "    FEARFUL = \"06\"\n",
    "    DISGUST = \"07\"\n",
    "    SURPRISED = \"08\"\n",
    "    EXCITED = \"09\"\n",
    "    PLEASURE = \"10\"\n",
    "    PAIN = \"11\"\n",
    "    DISAPPOINTMENT = \"12\"\n",
    "    BREATH = \"13\"\n",
    "\n",
    "\n",
    "class EmotionalIntensity(BaseEnum):\n",
    "\n",
    "    NORMAL = \"01\"\n",
    "    HIGH = \"02\"\n",
    "\n",
    "\n",
    "# the age of the person expressing an emotion\n",
    "class Age(BaseEnum):\n",
    "\n",
    "    ABOVE_65 = \"01\"\n",
    "    BETWEEN_20_AND_64 = \"02\"\n",
    "    BELOW_20 = \"03\"\n",
    "\n",
    "\n",
    "# the sex of the actor in the audio file\n",
    "class Sex(BaseEnum):\n",
    "\n",
    "    MALE = \"male\"\n",
    "    FEMALE = \"female\"\n",
    "\n",
    "\n",
    "# the source of the audio\n",
    "class Source(BaseEnum):\n",
    "\n",
    "    WEBSITE = \"01\"\n",
    "    YOUTUBE = \"02\"\n",
    "    MOVIES = \"03\"\n",
    "\n",
    "\n",
    "# the language spoken in the audio file\n",
    "class Language(BaseEnum):\n",
    "\n",
    "    CHINESE = \"01\"\n",
    "    ENGLISH = \"02\"\n",
    "    FRENCH = \"03\"\n",
    "    OTHER = \"others\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send, let's define a method that identifies wheather the actor is male or female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even number represents male, odd number represents female\n",
    "def get_sex_identifier(sex_identifier: int = None) -> Literal[Sex.MALE, Sex.FEMALE]:\n",
    "    return Sex.MALE if sex_identifier / 2 else Sex.FEMALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name_from_full_path(file_path: str = None) -> str:\n",
    "    return file_path.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third and last, let's define a method that returns all audio files based on the provided identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_files_by_identifiers(\n",
    "    files: List[str],\n",
    "    modality: List[Modality] = Modality.AUDIO_ONLY, \n",
    "    vocal_channel: VocalChannel = None,\n",
    "    emotion: List[Emotion] = None, \n",
    "    emotional_intensity: List[EmotionalIntensity] = None,\n",
    "    age: List[Age] = None,\n",
    "    source: List[Source] = None,\n",
    "    language: Language = Language.ENGLISH\n",
    ") -> list:\n",
    "    ret_val = list() # intialize the return value as an empty array\n",
    "\n",
    "    for index, file in enumerate(files):\n",
    "        # extract the file name from the full file path\n",
    "        file_name = get_file_name_from_full_path(file_path=file)\n",
    "\n",
    "        # remove the extension of the file name\n",
    "        file_name = file_name.replace(\".wav\", '')\n",
    "\n",
    "        # extract the identifiers from the file name\n",
    "        file_identifiers = file_name.split(\"-\")\n",
    "\n",
    "        # there are 99 cases where the \n",
    "        if len(file_identifiers) > 8:\n",
    "            # filter out the desired files by the criteria\n",
    "            if file_identifiers[0] in modality \\\n",
    "                and file_identifiers[1] in vocal_channel \\\n",
    "                and file_identifiers[2] in emotion \\\n",
    "                and file_identifiers[3] in emotional_intensity \\\n",
    "                and file_identifiers[6] in age \\\n",
    "                and file_identifiers[7] in source \\\n",
    "                and file_identifiers[8] == language:\n",
    "                    ret_val.append(file)\n",
    "\n",
    "    return ret_val\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to load in the data and see what we are working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/asvp-esd/Audio/actor_94/03-01-02-01-07-94-02-02-01-32.wav',\n",
       " '../../data/asvp-esd/Audio/actor_94/03-02-11-01-10-94-02-02-01-31.wav',\n",
       " '../../data/asvp-esd/Audio/actor_94/03-02-05-01-12-94-02-02-02-99.wav',\n",
       " '../../data/asvp-esd/Audio/actor_94/03-02-08-02-01-94-04-02-01-38.wav',\n",
       " '../../data/asvp-esd/Audio/actor_94/03-02-03-01-20-94-02-02-01-33.wav']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_files: List[str] = glob(\"../../data/asvp-esd/Audio/*/*.wav\")\n",
    "audio_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12625"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the length of the python list represents the number of audio files\n",
    "len(audio_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to filter the audio files by the criteria set previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech files\n",
    "\n",
    "## happy\n",
    "happy_speech = get_audio_files_by_identifiers(\n",
    "    files=audio_files, \n",
    "    modality=[Modality.AUDIO_ONLY], \n",
    "    vocal_channel=[VocalChannel.SPEECH], \n",
    "    emotion=[Emotion.HAPPY],\n",
    "    emotional_intensity=[EmotionalIntensity.NORMAL, EmotionalIntensity.HIGH],\n",
    "    age=[Age.BELOW_20, Age.BETWEEN_20_AND_64, Age.ABOVE_65],\n",
    "    source=[Source.MOVIES, Source.WEBSITE, Source.YOUTUBE],\n",
    ")\n",
    "\n",
    "## excited\n",
    "excited_speech = get_audio_files_by_identifiers(\n",
    "    files=audio_files, \n",
    "    modality=[Modality.AUDIO_ONLY], \n",
    "    vocal_channel=[VocalChannel.SPEECH], \n",
    "    emotion=[Emotion.EXCITED],\n",
    "    emotional_intensity=[EmotionalIntensity.NORMAL, EmotionalIntensity.HIGH],\n",
    "    age=[Age.BELOW_20, Age.BETWEEN_20_AND_64, Age.ABOVE_65],\n",
    "    source=[Source.MOVIES, Source.WEBSITE, Source.YOUTUBE],\n",
    ")\n",
    "\n",
    "## tender\n",
    "\n",
    "## scared\n",
    "scared_speech = get_audio_files_by_identifiers(\n",
    "    files=audio_files, \n",
    "    modality=[Modality.AUDIO_ONLY], \n",
    "    vocal_channel=[VocalChannel.SPEECH], \n",
    "    emotion=[Emotion.FEARFUL],\n",
    "    emotional_intensity=[EmotionalIntensity.NORMAL, EmotionalIntensity.HIGH],\n",
    "    age=[Age.BELOW_20, Age.BETWEEN_20_AND_64, Age.ABOVE_65],\n",
    "    source=[Source.MOVIES, Source.WEBSITE, Source.YOUTUBE],\n",
    ")\n",
    "\n",
    "## angry\n",
    "angry_speech = get_audio_files_by_identifiers(\n",
    "    files=audio_files, \n",
    "    modality=[Modality.AUDIO_ONLY], \n",
    "    vocal_channel=[VocalChannel.SPEECH], \n",
    "    emotion=[Emotion.ANGRY],\n",
    "    emotional_intensity=[EmotionalIntensity.NORMAL, EmotionalIntensity.HIGH],\n",
    "    age=[Age.BELOW_20, Age.BETWEEN_20_AND_64, Age.ABOVE_65],\n",
    "    source=[Source.MOVIES, Source.WEBSITE, Source.YOUTUBE],\n",
    ")\n",
    "\n",
    "## sad\n",
    "sad_speech = get_audio_files_by_identifiers(\n",
    "    files=audio_files, \n",
    "    modality=[Modality.AUDIO_ONLY], \n",
    "    vocal_channel=[VocalChannel.SPEECH], \n",
    "    emotion=[Emotion.SAD],\n",
    "    emotional_intensity=[EmotionalIntensity.NORMAL, EmotionalIntensity.HIGH],\n",
    "    age=[Age.BELOW_20, Age.BETWEEN_20_AND_64, Age.ABOVE_65],\n",
    "    source=[Source.MOVIES, Source.WEBSITE, Source.YOUTUBE],\n",
    ")\n",
    "\n",
    "\n",
    "# non speech files\n",
    "\n",
    "## happy\n",
    "happy_non_speech = get_audio_files_by_identifiers(\n",
    "    files=audio_files, \n",
    "    modality=[Modality.AUDIO_ONLY], \n",
    "    vocal_channel=[VocalChannel.NON_SPEECH], \n",
    "    emotion=[Emotion.HAPPY],\n",
    "    emotional_intensity=[EmotionalIntensity.NORMAL, EmotionalIntensity.HIGH],\n",
    "    age=[Age.BELOW_20, Age.BETWEEN_20_AND_64, Age.ABOVE_65],\n",
    "    source=[Source.MOVIES, Source.WEBSITE, Source.YOUTUBE],\n",
    ")\n",
    "\n",
    "## excited\n",
    "excited_non_speech = get_audio_files_by_identifiers(\n",
    "    files=audio_files, \n",
    "    modality=[Modality.AUDIO_ONLY], \n",
    "    vocal_channel=[VocalChannel.NON_SPEECH], \n",
    "    emotion=[Emotion.EXCITED],\n",
    "    emotional_intensity=[EmotionalIntensity.NORMAL, EmotionalIntensity.HIGH],\n",
    "    age=[Age.BELOW_20, Age.BETWEEN_20_AND_64, Age.ABOVE_65],\n",
    "    source=[Source.MOVIES, Source.WEBSITE, Source.YOUTUBE],\n",
    ")\n",
    "\n",
    "## tender\n",
    "\n",
    "## scared\n",
    "scared_non_speech = get_audio_files_by_identifiers(\n",
    "    files=audio_files, \n",
    "    modality=[Modality.AUDIO_ONLY], \n",
    "    vocal_channel=[VocalChannel.NON_SPEECH], \n",
    "    emotion=[Emotion.FEARFUL],\n",
    "    emotional_intensity=[EmotionalIntensity.NORMAL, EmotionalIntensity.HIGH],\n",
    "    age=[Age.BELOW_20, Age.BETWEEN_20_AND_64, Age.ABOVE_65],\n",
    "    source=[Source.MOVIES, Source.WEBSITE, Source.YOUTUBE],\n",
    ")\n",
    "\n",
    "## angry\n",
    "angry_non_speech = get_audio_files_by_identifiers(\n",
    "    files=audio_files, \n",
    "    modality=[Modality.AUDIO_ONLY], \n",
    "    vocal_channel=[VocalChannel.NON_SPEECH], \n",
    "    emotion=[Emotion.ANGRY],\n",
    "    emotional_intensity=[EmotionalIntensity.NORMAL, EmotionalIntensity.HIGH],\n",
    "    age=[Age.BELOW_20, Age.BETWEEN_20_AND_64, Age.ABOVE_65],\n",
    "    source=[Source.MOVIES, Source.WEBSITE, Source.YOUTUBE],\n",
    ")\n",
    "\n",
    "## sad\n",
    "sad_non_speech = get_audio_files_by_identifiers(\n",
    "    files=audio_files, \n",
    "    modality=[Modality.AUDIO_ONLY], \n",
    "    vocal_channel=[VocalChannel.NON_SPEECH], \n",
    "    emotion=[Emotion.SAD],\n",
    "    emotional_intensity=[EmotionalIntensity.NORMAL, EmotionalIntensity.HIGH],\n",
    "    age=[Age.BELOW_20, Age.BETWEEN_20_AND_64, Age.ABOVE_65],\n",
    "    source=[Source.MOVIES, Source.WEBSITE, Source.YOUTUBE],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196, 127, 56, 265, 109)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(happy_speech), len(excited_speech), len(scared_speech), len(angry_speech), len(sad_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(438, 94, 467, 297, 272)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(happy_non_speech), len(excited_non_speech), len(scared_non_speech), len(angry_non_speech), len(sad_non_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base path\n",
    "BASE_FILEPATH = \"../../data/asvp-esd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(path=\"../../data/asvp-esd/sound\", ignore_errors=True)\n",
    "shutil.rmtree(path=\"../../data/asvp-esd/speech\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize folders to save files to\n",
    "for _, vocal_channel in enumerate([\"sound\", \"speech\"]):\n",
    "    for _, emotion in enumerate([\"happy\", \"excited\", \"tender\", \"scared\", \"angry\", \"sad\"]):\n",
    "        pathlib.Path(f\"{BASE_FILEPATH}/{vocal_channel}/{emotion}\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to save new sub sets of audio files\n",
    "def save_audio(audio_files: List[str] = None, vocal_channel: str = None, emotion: str = None):\n",
    "    for _, audio_file in enumerate(audio_files):\n",
    "        # load the waveform and sample rate of the audio file\n",
    "        waveform, sr = torchaudio.backend.sox_io_backend.load(audio_file)\n",
    "\n",
    "        # specify new file path & name, provide waveform & sample rate\n",
    "        torchaudio.backend.sox_io_backend.save(\n",
    "            filepath=f\"{BASE_FILEPATH}/{vocal_channel}/{emotion}/{uuid4()}.wav\", \n",
    "            src=waveform, \n",
    "            sample_rate=sr\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "## happy\n",
    "save_audio(audio_files=happy_speech, vocal_channel=\"speech\", emotion=\"happy\")\n",
    "save_audio(audio_files=happy_non_speech, vocal_channel=\"sound\", emotion=\"happy\")\n",
    "\n",
    "# excited\n",
    "save_audio(audio_files=excited_speech, vocal_channel=\"speech\", emotion=\"excited\")\n",
    "save_audio(audio_files=excited_non_speech, vocal_channel=\"sound\", emotion=\"excited\")\n",
    "\n",
    "# tender\n",
    "\n",
    "# scared\n",
    "save_audio(audio_files=scared_speech, vocal_channel=\"speech\", emotion=\"scared\")\n",
    "save_audio(audio_files=scared_non_speech, vocal_channel=\"sound\", emotion=\"scared\")\n",
    "\n",
    "# angry\n",
    "save_audio(audio_files=angry_speech, vocal_channel=\"speech\", emotion=\"angry\")\n",
    "save_audio(audio_files=angry_non_speech, vocal_channel=\"sound\", emotion=\"angry\")\n",
    "\n",
    "# sad\n",
    "save_audio(audio_files=sad_speech, vocal_channel=\"speech\", emotion=\"sad\")\n",
    "save_audio(audio_files=sad_non_speech, vocal_channel=\"sound\", emotion=\"sad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveform(waveform: torch.Tensor, sr: int, title: str = \"Waveform\") -> None:\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.range(0, num_frames) / sr\n",
    "\n",
    "    figure, axes = plt.subplot(num_channels, 1)\n",
    "    axes.plot(time_axis, waveform[0], linewidth=1)\n",
    "    axes.grid(True)\n",
    "\n",
    "    figure.suptitle(title)\n",
    "\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "def plot_spectogram(specgram, title: str = \"Spectogram (db)\", ylabel=\"freq_bin\") -> None:\n",
    "    fig, axs = plt.subplot(1, 1)\n",
    "\n",
    "    axs.set_title(title)\n",
    "    axs.ylabel(ylabel)\n",
    "    axs.xlabel(\"frame\")\n",
    "\n",
    "    im = axs.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\")\n",
    "    fig.colorbar(im, ax=axs)\n",
    "\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "def plot_fbank(fbank, title: str = \"Filter bank\") -> None:\n",
    "    fig, axs = plt.subplot(1, 1)\n",
    "\n",
    "    axs.set_title(title)\n",
    "    axs.imshow(fbank, aspect=\"auto\")\n",
    "    axs.set_ylabel(\"freq_bin\")\n",
    "    axs.set_xlabel(\"mel_bin\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files: List[str] = glob(\"../../data/asvp-esd/speech/*/*.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "753"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files: List[str] = glob(\"../../data/asvp-esd/sound/*/*.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('empathic-art-VdJ2KSTr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e50c87813b58f42c33168ff8a825bf8dc8f12ff2146cb51a790b6579912f82a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
